{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separately collects unique subject name from files name\n",
    "def getFilename(full_dir):\n",
    "    _,filename = full_dir.split('\\\\')\n",
    "    print(filename)\n",
    "    subject,_,_,_,_= filename.split('_')\n",
    "    return str(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created empty arrays to collects by the categories\n",
    "segm_byInst = []\n",
    "\n",
    "far_flair_byInst = []\n",
    "far_t1_byInst = []\n",
    "far_t1ce_byInst = []\n",
    "far_t2_byInst = []\n",
    "far_adc_byInst = []\n",
    "\n",
    "Near_flair_byinst = []\n",
    "Near_t1_byinst = []\n",
    "Near_t1ce_byinst = []\n",
    "Near_t2_byinst = []\n",
    "Near_adc_byinst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call first institution subjects\n",
    "first_seg_dir = 'segmentation images directory'\n",
    "first_far_dir = 'collected Far patches'\n",
    "first_near_dir = 'collected Near patches'\n",
    "\n",
    "# Collects all the files for first institutions\n",
    "first_segm_dir = sorted(glob.glob(os.path.join(first_seg_dir, \"*_segm*.nii.gz\")))\n",
    "\n",
    "first_far_flair_dir = sorted(glob.glob(os.path.join(first_far_dir, \"*_flairPatch*.nii.gz\"))) \n",
    "first_far_t1_dir = sorted(glob.glob(os.path.join(first_far_dir, \"*_t1Patch*.nii.gz\"))) \n",
    "first_far_t1ce_dir = sorted(glob.glob(os.path.join(first_far_dir, \"*_t1cePatch*.nii.gz\"))) \n",
    "first_far_t2_dir = sorted(glob.glob(os.path.join(first_far_dir, \"*_t2Patch*.nii.gz\"))) \n",
    "first_far_adc_dir = sorted(glob.glob(os.path.join(first_far_dir, \"*_adcPatch*.nii.gz\"))) \n",
    "\n",
    "first_near_flair_dir = sorted(glob.glob(os.path.join(first_near_dir, \"*_flairPatch*.nii.gz\"))) \n",
    "first_near_t1_dir = sorted(glob.glob(os.path.join(first_near_dir, \"*_t1Patch*.nii.gz\"))) \n",
    "first_near_t1ce_dir = sorted(glob.glob(os.path.join(first_near_dir, \"*_t1cePatch*.nii.gz\"))) \n",
    "first_near_t2_dir = sorted(glob.glob(os.path.join(first_near_dir, \"*_t2Patch*.nii.gz\"))) \n",
    "first_near_adc_dir = sorted(glob.glob(os.path.join(first_near_dir, \"*_adcPatch*.nii.gz\"))) \n",
    "\n",
    "# Added to the array by categories\n",
    "segm_byInst.append(first_segm_dir)\n",
    "\n",
    "far_flair_byInst.append(first_far_flair_dir)\n",
    "far_t1_byInst.append(first_far_t1_dir)\n",
    "far_t1ce_byInst.append(first_far_t1ce_dir)\n",
    "far_t2_byInst.append(first_far_t2_dir)\n",
    "far_adc_byInst.append(first_far_adc_dir)\n",
    "\n",
    "Near_flair_byinst.append(first_near_flair_dir)\n",
    "Near_t1_byinst.append(first_near_t1_dir)\n",
    "Near_t1ce_byinst.append(first_near_t1ce_dir)\n",
    "Near_t2_byinst.append(first_near_t2_dir)\n",
    "Near_adc_byinst.append(first_near_adc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call second institutino subjects\n",
    "second_seg_dir = 'segmentation images directory'\n",
    "second_far_dir = 'collected Far patches'\n",
    "second_near_dir = 'collected Near patches'\n",
    "\n",
    "# Collects all the files for second institutions\n",
    "second_segm_dir = sorted(glob.glob(os.path.join(second_seg_dir, \"*_segm*.nii.gz\")))\n",
    "\n",
    "second_far_flair_dir = sorted(glob.glob(os.path.join(second_far_dir, \"*_flairPatch*.nii.gz\"))) \n",
    "second_far_t1_dir = sorted(glob.glob(os.path.join(second_far_dir, \"*_t1Patch*.nii.gz\"))) \n",
    "second_far_t1ce_dir = sorted(glob.glob(os.path.join(second_far_dir, \"*_t1cePatch*.nii.gz\"))) \n",
    "second_far_t2_dir = sorted(glob.glob(os.path.join(second_far_dir, \"*_t2Patch*.nii.gz\"))) \n",
    "second_far_adc_dir = sorted(glob.glob(os.path.join(second_far_dir, \"*_adcPatch*.nii.gz\"))) \n",
    "\n",
    "second_near_flair_dir = sorted(glob.glob(os.path.join(second_near_dir, \"*_flairPatch*.nii.gz\"))) \n",
    "second_near_t1_dir = sorted(glob.glob(os.path.join(second_near_dir, \"*_t1Patch*.nii.gz\"))) \n",
    "second_near_t1ce_dir = sorted(glob.glob(os.path.join(second_near_dir, \"*_t1cePatch*.nii.gz\"))) \n",
    "second_near_t2_dir = sorted(glob.glob(os.path.join(second_near_dir, \"*_t2Patch*.nii.gz\"))) \n",
    "second_near_adc_dir = sorted(glob.glob(os.path.join(second_near_dir, \"*_adcPatch*.nii.gz\")))\n",
    "\n",
    "segm_byInst.append(second_segm_dir)\n",
    "\n",
    "far_flair_byInst.append(second_far_flair_dir)\n",
    "far_t1_byInst.append(second_far_t1_dir)\n",
    "far_t1ce_byInst.append(second_far_t1ce_dir)\n",
    "far_t2_byInst.append(second_far_t2_dir)\n",
    "far_adc_byInst.append(second_far_adc_dir)\n",
    "\n",
    "Near_flair_byinst.append(second_near_flair_dir)\n",
    "Near_t1_byinst.append(second_near_t1_dir)\n",
    "Near_t1ce_byinst.append(second_near_t1ce_dir)\n",
    "Near_t2_byinst.append(second_near_t2_dir)\n",
    "Near_adc_byinst.append(second_near_adc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may add more institutinos as you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nifti files\n",
    "def LoadingImage(dir):\n",
    "    \n",
    "    nifti_image = nib.load(dir)\n",
    "    image = np.asarray(nifti_image.dataobj)\n",
    "    header = nifti_image.header\n",
    "    imgaffine = nifti_image.affine\n",
    "    \n",
    "    return image, header, imgaffine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and stack them for training to sahpe of (5,5,5,5) \n",
    "def load_and_stack_image_dirs(dirs):\n",
    "    stacked_images = []\n",
    "    for i in range(len(dirs[0])):  # Assume all lists have same length\n",
    "        # Load images from each directory, then stack along a new dimension\n",
    "        images = [nib.load(dir[i]).get_fdata() for dir in dirs]\n",
    "        for image in images:\n",
    "            if image.shape != (5, 5, 5):\n",
    "                print(f'error: ' + dirs[0][i])\n",
    "                print(image.shape)\n",
    "        stacked_image = np.stack(images, axis=0)  # Now each image is treated as a separate channel\n",
    "        stacked_images.append(stacked_image)\n",
    "\n",
    "    return np.array(stacked_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 3D CNN network\n",
    "class Simple3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Simple3DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(5, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5 * 5, 128)  # 128 is an arbitrary choice, feel free to change\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))  # apply conv1, then ReLU\n",
    "        x = torch.relu(self.conv2(x))  # apply conv2, then ReLU\n",
    "        x = x.view(x.size(0), -1)  # flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))  # apply first fully connected layer, then ReLU\n",
    "        x = self.dropout(x)  # apply dropout\n",
    "        x = self.fc2(x)  # apply second fully connected layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['first_institution', 'second_institution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of institutions you use\n",
    "n = 2\n",
    "\n",
    "# Iterate by each institution and perform leave-one-site-out training. \n",
    "for k in range(n):\n",
    "    segm_combine = segm_byInst[:k] + segm_byInst[k+1:]\n",
    "\n",
    "    far_flair_combine = far_flair_byInst[:k] + far_flair_byInst[k+1:]\n",
    "    far_t1_combine = far_t1_byInst[:k] + far_t1_byInst[k+1:]\n",
    "    far_t1ce_combine = far_t1ce_byInst[:k] + far_t1ce_byInst[k+1:]\n",
    "    far_t2_combine = far_t2_byInst[:k] + far_t2_byInst[k+1:]\n",
    "    far_adc_combine = far_adc_byInst[:k] + far_adc_byInst[k+1:]\n",
    "\n",
    "    Near_flair_combine = Near_flair_byinst[:k] + Near_flair_byinst[k+1:]\n",
    "    Near_t1_combine = Near_t1_byinst[:k] + Near_t1_byinst[k+1:]\n",
    "    Near_t1ce_combine = Near_t1ce_byinst[:k] + Near_t1ce_byinst[k+1:]\n",
    "    Near_t2_combine = Near_t2_byinst[:k] + Near_t2_byinst[k+1:]\n",
    "    Near_adc_combine = Near_adc_byinst[:k] + Near_adc_byinst[k+1:]\n",
    "\n",
    "    far_flair_dir = list(itertools.chain.from_iterable(far_flair_combine))\n",
    "    far_t1_dir = list(itertools.chain.from_iterable(far_t1_combine))\n",
    "    far_t1ce_dir = list(itertools.chain.from_iterable(far_t1ce_combine))\n",
    "    far_t2_dir = list(itertools.chain.from_iterable(far_t2_combine))\n",
    "    far_adc_dir = list(itertools.chain.from_iterable(far_adc_combine))\n",
    "\n",
    "    rec_flair_dir = list(itertools.chain.from_iterable(Near_flair_combine))\n",
    "    rec_t1_dir = list(itertools.chain.from_iterable(Near_t1_combine))\n",
    "    rec_t1ce_dir = list(itertools.chain.from_iterable(Near_t1ce_combine))\n",
    "    rec_t2_dir = list(itertools.chain.from_iterable(Near_t2_combine))\n",
    "    rec_adc_dir = list(itertools.chain.from_iterable(Near_adc_combine))\n",
    "\n",
    "    far_dirs = [far_flair_dir, far_t1_dir, far_t1ce_dir, far_t2_dir, far_adc_dir]\n",
    "    rec_dirs = [rec_flair_dir, rec_t1_dir, rec_t1ce_dir, rec_t2_dir, rec_adc_dir]\n",
    "\n",
    "    #(N,C,H,W,D)\n",
    "    far_images = load_and_stack_image_dirs(far_dirs)\n",
    "    rec_images = load_and_stack_image_dirs(rec_dirs)\n",
    "\n",
    "    if rec_images.shape[0] > far_images.shape[0]:\n",
    "        indices = np.random.choice(rec_images.shape[0], far_images.shape[0], replace=False)\n",
    "        rec_images = rec_images[indices]\n",
    "    else:\n",
    "        indices = np.random.choice(far_images.shape[0], rec_images.shape[0], replace=False)\n",
    "        far_images = far_images[indices]\n",
    "\n",
    "    all_images = np.concatenate((far_images, rec_images), axis=0)\n",
    "    print(all_images.shape)\n",
    "\n",
    "    y = np.concatenate([np.zeros(len(far_images)), np.ones(len(rec_images))], axis=0)  # Replace with your actual labels\n",
    "    print(y.shape)\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    x_train, x_val, y_train, y_val = train_test_split(all_images, y, test_size=0.3, random_state=42, shuffle = True)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    x_train = torch.from_numpy(x_train).float()\n",
    "    y_train = torch.from_numpy(y_train).long()\n",
    "    x_val = torch.from_numpy(x_val).float()\n",
    "    y_val = torch.from_numpy(y_val).long()\n",
    "\n",
    "    # Create DataLoaders\n",
    "    trainset = TensorDataset(x_train, y_train)\n",
    "    valset = TensorDataset(x_val, y_val)\n",
    "    trainloader = DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=16, shuffle=False)\n",
    "\n",
    "    # Define the model, loss function, and optimizer\n",
    "    model = Simple3DCNN(num_classes=2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "    # Lists for storing losses and accuracies\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Variables for early stopping\n",
    "    best_val_loss = float('inf')  # set initial best validation loss to infinity\n",
    "    patience = 10\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 1000\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_losses.append(train_loss / len(trainloader))  # Compute average loss\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            val_accuracy = correct / total\n",
    "            val_accuracies.append(val_accuracy)  # Store validation accuracy\n",
    "        val_losses.append(val_loss / len(valloader))  # Compute average loss\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:  # check if the current validation loss is lower than the best validation loss\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the model weights\n",
    "            torch.save(model.state_dict(), 'Save model in ouput directory')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_losses[-1]}, Validation Loss: {val_losses[-1]}, Validation Accuracy: {val_accuracy * 100}%')\n",
    "\n",
    "    # Plot training and validation losses\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training loss')\n",
    "    plt.plot(val_losses, label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracies, label='Validation accuracy')\n",
    "    plt.title('Validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
